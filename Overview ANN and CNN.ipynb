{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN and CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOcXP5+Lxidn+9w2cmjUQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleysonl/Tensorflow2.0/blob/master/Overview%20ANN%20and%20CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oMkAdfqXowa",
        "colab_type": "text"
      },
      "source": [
        "# **ANN with TF2.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGWvRnzShlC0",
        "colab_type": "code",
        "outputId": "a95fcd87-9cde-41f0-ed5d-616a533b29f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0.alpha0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0.alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.8.1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.17.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.1.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0.alpha0) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0.alpha0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0.alpha0) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0.alpha0) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jON5df32k-jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTkqVU6JmizA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # or any {'0',' 1', '2'}\n",
        "\n",
        "def mnist_dataset():\n",
        "  (x,y),_ = tf.keras.datasets.mnist.load_data()\n",
        "  ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "  ds = ds.map(prepare_mnist_features_and_labels)\n",
        "  ds = ds.take(20000).shuffle(20000).batch(100)\n",
        "  return ds\n",
        "\n",
        "def prepare_mnist_features_and_labels(x,y):\n",
        "  x = tf.cast(x, tf.float32)/255.\n",
        "  y = tf.cast(y, tf.int64)\n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs1XfaIQlSHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the model\n",
        "model = tf.keras.models.Sequential()\n",
        "# Define the layers\n",
        "model.add(tf.keras.layers.Reshape(target_shape=(28*28,),input_shape=(28,28)))\n",
        "model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=10))\n",
        "#define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVssbUmir16f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def compute_loss(logits, labels):\n",
        "  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))\n",
        "\n",
        "@tf.function\n",
        "def compute_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32)) \n",
        "\n",
        "@tf.function\n",
        "def train_one_step(model, optimizer, x, y):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x)\n",
        "    loss = compute_loss(logits, y)\n",
        "\n",
        "    #compute gradient\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    #update weights\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    accuracy = compute_accuracy(logits, y)\n",
        "\n",
        "    #loss and accuracy is scalar tensor\n",
        "    return loss, accuracy\n",
        "\n",
        "def train(epoch, model, optimizer):\n",
        "  train_ds = mnist_dataset()\n",
        "  loss = 0.0\n",
        "  accuracy = 0.0\n",
        "  for step, (x,y) in enumerate(train_ds):\n",
        "    loss, accuracy = train_one_step(model, optimizer, x, y)\n",
        "\n",
        "    if step%500 == 0:\n",
        "      print('epoch', epoch, ':loss', loss.numpy(), ': accuracy', accuracy.numpy())\n",
        "\n",
        "  return loss, accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiaOjS9B1dGz",
        "colab_type": "code",
        "outputId": "033c9d96-aeac-4ca5-cfe3-2eb4ef99e2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "for epoch in range(20):\n",
        "  loss, accuracy = train(epoch, model, optimizer)\n",
        "\n",
        "print('Final epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 :loss 2.3029244 : accuracy 0.13\n",
            "epoch 1 :loss 0.16009605 : accuracy 0.95\n",
            "epoch 2 :loss 0.17455128 : accuracy 0.95\n",
            "epoch 3 :loss 0.086833164 : accuracy 0.98\n",
            "epoch 4 :loss 0.09488362 : accuracy 0.96\n",
            "epoch 5 :loss 0.037679218 : accuracy 1.0\n",
            "epoch 6 :loss 0.049664084 : accuracy 0.98\n",
            "epoch 7 :loss 0.029889045 : accuracy 1.0\n",
            "epoch 8 :loss 0.008005618 : accuracy 1.0\n",
            "epoch 9 :loss 0.028494503 : accuracy 0.99\n",
            "epoch 10 :loss 0.013228629 : accuracy 1.0\n",
            "epoch 11 :loss 0.01597538 : accuracy 1.0\n",
            "epoch 12 :loss 0.00436632 : accuracy 1.0\n",
            "epoch 13 :loss 0.0071314042 : accuracy 1.0\n",
            "epoch 14 :loss 0.007745029 : accuracy 1.0\n",
            "epoch 15 :loss 0.0046918783 : accuracy 1.0\n",
            "epoch 16 :loss 0.004132434 : accuracy 1.0\n",
            "epoch 17 :loss 0.0023726213 : accuracy 1.0\n",
            "epoch 18 :loss 0.003863144 : accuracy 1.0\n",
            "epoch 19 :loss 0.0038915456 : accuracy 1.0\n",
            "Final epoch 19 : loss 0.0015582936 ; accuracy 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xq5hRsh1kf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEDQidO1Xusy",
        "colab_type": "text"
      },
      "source": [
        "#**CNN with TF2.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-t8-0wxXyUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models, optimizers, metrics\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loGxDJ26c-gT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mnist_dataset():\n",
        "  (X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "  X_train, X_test = X_train/np.float32(255), X_test/np.float32(255)\n",
        "  y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "train_ds, test_ds = mnist_dataset()\n",
        "train_ds = train_ds.shuffle(60000).batch(100)\n",
        "test_ds = test_ds.batch(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrkEJK3Rffkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Reshape(target_shape=[28,28,1], input_shape=(28, 28,)))\n",
        "model.add(layers.Conv2D(filters = 2, kernel_size = 5,padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size = (2,2), strides= (2,2), padding='same',))\n",
        "model.add(layers.Conv2D(filters = 4, kernel_size = 5, padding ='same'))\n",
        "model.add(layers.MaxPooling2D(pool_size = (2,2), strides= (2,2), padding='same',))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(units = 32, activation='relu'))\n",
        "model.add(layers.Dropout(rate=0.4))\n",
        "model.add(layers.Dense(units=10))\n",
        "\n",
        "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "compute_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy()\n",
        "optimizer = optimizers.SGD(learning_rate=0.01, momentum = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUNPCWoTlvpX",
        "colab_type": "text"
      },
      "source": [
        "In addition to the training procedure, we'll also write test() a method for evaluation, and use **tf.python.summary_ops_v2** to record training summaries to TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcIyHNJBiiPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(model, optimizer, images, labels):\n",
        "\n",
        "  \n",
        "    # Record the operations used to compute the loss, so that the gradient\n",
        "    # of the loss with respect to the variables can be computed\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images, training=True)\n",
        "    loss = compute_loss(labels, logits)\n",
        "    compute_accuracy(labels, logits)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  \n",
        "  return loss\n",
        "\n",
        "def train(model, optimizer, dataset, log_freq=50):\n",
        "  # Metrics are stateful. They accumulate values and return a cumulative\n",
        "  # result when you call .result(). Clear accumulated values with .reset_states()\n",
        "  avg_loss = metrics.Mean('loss', dtype= tf.float32)\n",
        "\n",
        "  for images, labels in dataset:\n",
        "    loss = train_step(model, optimizer, images, labels)\n",
        "    avg_loss(loss)\n",
        "\n",
        "    if tf.equal(optimizer.iterations % log_freq, 0):\n",
        "      summary_ops_v2.scalar('loss', avg_loss.result(), step=optimizer.iterations)\n",
        "      summary_ops_v2.scalar('accuracy', compute_accuracy.result(), step=optimizer.iterations)\n",
        "      print('step:', int(optimizer.iterations),\n",
        "            'loss:', avg_loss.result().numpy(),\n",
        "            'acc:', compute_accuracy.result().numpy())\n",
        "      avg_loss.reset_states()\n",
        "      compute_accuracy.reset_states()\n",
        "\n",
        "def test(model, dataset, step_num):\n",
        "  avg_loss = metrics.Mean('loss', dtype=tf.float32)\n",
        "  \n",
        "  for(images, labels) in dataset:\n",
        "    logits = model(images, training=False)\n",
        "    avg_loss(compute_loss(labels, logits))\n",
        "    compute_accuracy(labels, logits)\n",
        "\n",
        "  print('Model test set loss: {:0.4f} accuracy: {:0.2f}%'.format(avg_loss.result(), compute_accuracy.result()*100))\n",
        "\n",
        "  print('loss:', avg_loss.result(), 'acc:', compute_accuracy.result())\n",
        "  summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "  summary_ops_v2.scalar('accuracy', compute_accuracy.result(), step=step_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrG7p_vll4w",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our data, model, and training procedure ready, we just need to designate a directory and create a **tf.train.Checkpoint** file to save our parameters to as we train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STpnljQtlroO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb191186-8b0b-4496-aeec-aabaf385c729"
      },
      "source": [
        "# where to save checkpoints, tensorboard summaries, etc.\n",
        "MODEL_DIR = '/tmp/tensorflow/mnist'\n",
        "\n",
        "def apply_clean():\n",
        "  if tf.io.gfile.exists(MODEL_DIR):\n",
        "    print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
        "    tf.io.gfile.rmtree(MODEL_DIR)\n",
        "\n",
        "apply_clean()\n",
        "\n",
        "checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model = model, optimizer = optimizer)\n",
        "\n",
        "# Restore variables on creation if a checkpoint exists\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing existing model dir: /tmp/tensorflow/mnist\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fc6b83d3cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdYzAfqrppaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6e23071b-2a3b-48e0-eda2-12d5c7323343"
      },
      "source": [
        "NUM_TRAIN_EPOCHS = 5\n",
        "\n",
        "for i in range(NUM_TRAIN_EPOCHS):\n",
        "  start = time.time()\n",
        "  train(model, optimizer, train_ds, log_freq=500)\n",
        "  end = time.time()\n",
        "  print('Train time for epoch #{} ({} total steps): {}'.format(i + 1, int(optimizer.iterations), end - start))\n",
        "  #   with test_summary_writer.as_default():\n",
        "  #     test(model, test_ds, optimizer.iterations)\n",
        "  checkpoint.save(checkpoint_prefix)\n",
        "  print('saved checkpoint.')\n",
        "\n",
        "export_path = os.path.join(MODEL_DIR, 'export')\n",
        "tf.saved_model.save(model, export_path)\n",
        "print('saved SavedModel for exporting.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 3500 loss: 0.37016273 acc: 0.99269927\n",
            "Train time for epoch #1 (3600 total steps): 7.767822027206421\n",
            "saved checkpoint.\n",
            "step: 4000 loss: 0.34839368 acc: 0.99351925\n",
            "Train time for epoch #2 (4200 total steps): 7.887409925460815\n",
            "saved checkpoint.\n",
            "step: 4500 loss: 0.32761002 acc: 0.99441946\n",
            "Train time for epoch #3 (4800 total steps): 7.777082443237305\n",
            "saved checkpoint.\n",
            "step: 5000 loss: 0.31078783 acc: 0.99467963\n",
            "Train time for epoch #4 (5400 total steps): 7.786827325820923\n",
            "saved checkpoint.\n",
            "step: 5500 loss: 0.3073665 acc: 0.99454\n",
            "step: 6000 loss: 0.28918484 acc: 0.9955195\n",
            "Train time for epoch #5 (6000 total steps): 7.72770094871521\n",
            "saved checkpoint.\n",
            "INFO:tensorflow:Assets written to: /tmp/tensorflow/mnist/export/assets\n",
            "saved SavedModel for exporting.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtMwcl6nqPg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}